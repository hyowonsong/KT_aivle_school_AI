# ğŸš— ì°¨ëŸ‰ ê³µìœ ì—…ì²´ì˜ ì°¨ëŸ‰ íŒŒì† ì—¬ë¶€ ë¶„ë¥˜í•˜ê¸°(4ì°¨ë¯¸ë‹ˆí”„ë¡œì íŠ¸)
---

## **ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”**

- **ëª©ì :** ìŠ¤ë§ˆíŠ¸í° ì„¼ì„œ ë°ì´í„°ë¥¼ í™œìš©í•´ ì‚¬ìš©ìì˜ ëª¨ì…˜ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•
- **ê¸°ê°„:** 2024.04.09 ~ 2024.04.12
- **ë°ì´í„°:** **Car_Images.zip**: ì°¨ëŸ‰ì˜ ì •ìƒ/íŒŒì† ì´ë¯¸ì§€ ë¬´ì‘ìœ„ ìˆ˜ì§‘
- **ì‚¬ìš© ë„êµ¬:** Python, Pandas, Matplotlib, Seaborn, Scikit-learn

---

### ğŸ§¹ **ë°ì´í„° ì „ì²˜ë¦¬**

### ê³¼ì œ ìˆ˜í–‰ ëª©í‘œ

- **ë°ì´í„°ë¥¼ ëª¨ë¸ë§ì— ì í•©í•œ í˜•íƒœë¡œ ì •ë¦¬í•˜ê¸°**

### ì‘ì—… ë‹¨ê³„ ë° ì½”ë“œ

1. **ë°ì´í„° ì „ì²˜ë¦¬**

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img, img_to_array

def load_data(image_paths, label):
    data = []
    labels = []
    for img_path in image_paths:
        img = load_img(img_path, target_size=(128, 128))
        img = img_to_array(img)
        data.append(img)
        labels.append(label)
    return np.array(data), np.array(labels)

normal_paths = [f'/content/car_images/normal/{img}' for img in normal_images]
damaged_paths = [f'/content/car_images/damaged/{img}' for img in damaged_images]

X_normal, y_normal = load_data(normal_paths, 0)
X_damaged, y_damaged = load_data(damaged_paths, 1)

X = np.concatenate([X_normal, X_damaged])
y = np.concatenate([y_normal, y_damaged])

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.1, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

```

---

### ğŸ¤– ë¯¸ì…˜ 2: CNN ëª¨ë¸ë§

### ê³¼ì œ ìˆ˜í–‰ ëª©í‘œ

- **Kerasë¥¼ ì´ìš©í•˜ì—¬ 3ê°œ ì´ìƒì˜ ëª¨ë¸ ìƒì„± ë° ì„±ëŠ¥ ë¹„êµ**

### ëª¨ë¸ 1: ê¸°ë³¸ CNN

- **í›ˆë ¨ ì •í™•ë„**: 87.2%
- **ê²€ì¦ ì •í™•ë„**: 84.5%
- **í›ˆë ¨ ì†ì‹¤**: 0.35
- **ê²€ì¦ ì†ì‹¤**: 0.42

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping

model1 = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

history1 = model1.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), callbacks=[early_stopping])

```

### ëª¨ë¸ 2: Learning Rate ì¡°ì •

- **í›ˆë ¨ ì •í™•ë„**: 89.1%
- **ê²€ì¦ ì •í™•ë„**: 86.3%
- **í›ˆë ¨ ì†ì‹¤**: 0.30
- **ê²€ì¦ ì†ì‹¤**: 0.37

```python
from tensorflow.keras.optimizers import Adam

model2 = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

optimizer = Adam(learning_rate=0.0005)
model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
history2 = model2.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), callbacks=[early_stopping])

```

### ëª¨ë¸ 3: Deep CNN with Dropout

- **í›ˆë ¨ ì •í™•ë„**: 90.4%
- **ê²€ì¦ ì •í™•ë„**: 88.2%
- **í›ˆë ¨ ì†ì‹¤**: 0.28
- **ê²€ì¦ ì†ì‹¤**: 0.33

```python
from tensorflow.keras.layers import Dropout

model3 = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

optimizer = Adam(learning_rate=0.0001)
model3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
history3 = model3.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), callbacks=[early_stopping])

```

---

### ğŸ”„ ë¯¸ì…˜ 3: Transfer Learning

### ê³¼ì œ ìˆ˜í–‰ ëª©í‘œ

- **ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ë‘ ê°€ì§€ ì‹œë„**

### ì‘ì—… ë‹¨ê³„ ë° ì½”ë“œ

1. **Image Preprocessing Layer & Image Augmentation Layer**

```python
from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

datagen.fit(X_train)
```

1. **Transfer Learning (Inception V3)**
- **í›ˆë ¨ ì •í™•ë„**: 93.6%
- **ê²€ì¦ ì •í™•ë„**: 91.2%
- **í›ˆë ¨ ì†ì‹¤**: 0.22
- **ê²€ì¦ ì†ì‹¤**: 0.29

```python
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.models import Model

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)

model_transfer = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model_transfer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_transfer = model_transfer.fit(datagen.flow(X_train, y_train, batch_size=32),
                                       epochs=20,
                                       validation_data=(X_val, y_val),
                                       callbacks=[early_stopping])
plot_history(history_transfer, 'Transfer Learning Model')
```

## ğŸ” **ê²°ë¡ **

- ê²°ê³¼ë¥¼ ì¢…í•©í•´ ë³´ë©´, **Transfer Learning**ì„ ì ìš©í•œ ëª¨ë¸ì´ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.
- ë”°ë¼ì„œ **Transfer Learning**ì„ ì ìš©í•œ ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ ì„ íƒì…ë‹ˆë‹¤.

### ì„±ëŠ¥ ìš”ì•½

| ëª¨ë¸ | í›ˆë ¨ ì •í™•ë„ | ê²€ì¦ ì •í™•ë„ | í›ˆë ¨ ì†ì‹¤ | ê²€ì¦ ì†ì‹¤ |
| --- | --- | --- | --- | --- |
| ê¸°ë³¸ CNN | 87.2% | 84.5% | 0.35 | 0.42 |
| Learning Rate ì¡°ì • | 89.1% | 86.3% | 0.30 | 0.37 |
| Deep CNN with Dropout | 90.4% | 88.2% | 0.28 | 0.33 |
| Transfer Learning | 93.6% | 91.2% | 0.22 | 0.29 |
